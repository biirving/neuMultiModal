{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b88a516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/Desktop/ml/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModel\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from transformers import BeitFeatureExtractor, BeitForImageClassification, Trainer\n",
    "import numpy as np\n",
    "from datasets import Features, ClassLabel, Array3D, Image\n",
    "import torch\n",
    "from torch import nn, tensor\n",
    "import os\n",
    "from torchmetrics import Accuracy, MatthewsCorrCoef\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5777be4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/benjamin/.cache/huggingface/datasets/Multimodal-Fatima___parquet/Multimodal-Fatima--Hatefulmemes_train-cf2bb543f5aaeaee/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 443.89it/s]\n",
      "Found cached dataset parquet (/home/benjamin/.cache/huggingface/datasets/Multimodal-Fatima___parquet/Multimodal-Fatima--Hatefulmemes_test-c1760e361ffe8410/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 857.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "model = AutoModelForPreTraining.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "\n",
    "\"\"\"\n",
    "@article{DBLP:journals/corr/abs-2005-04790,\n",
    "  author    = {Douwe Kiela and\n",
    "               Hamed Firooz and\n",
    "               Aravind Mohan and\n",
    "               Vedanuj Goswami and\n",
    "               Amanpreet Singh and\n",
    "               Pratik Ringshia and\n",
    "               Davide Testuggine},\n",
    "  title     = {The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/2005.04790},\n",
    "  year      = {2020},\n",
    "  url       = {https://arxiv.org/abs/2005.04790},\n",
    "  eprinttype = {arXiv},\n",
    "  eprint    = {2005.04790},\n",
    "  timestamp = {Thu, 14 May 2020 16:56:02 +0200},\n",
    "  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-04790.bib},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "\"\"\"\n",
    "train_data = load_dataset(\"Multimodal-Fatima/Hatefulmemes_train\")\n",
    "test_data = load_dataset(\"Multimodal-Fatima/Hatefulmemes_test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d260c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the image for input\n",
    "my_transforms = transforms.Compose([                                                                                                                                                                                                \n",
    "transforms.Resize((3, 224, 224)),                                                                                                  \n",
    "transforms.ToTensor()                                                                                                           \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d04172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dandelin/vilt-b32-mlm were not used when initializing ViltModel: ['mlm_score.transform.LayerNorm.bias', 'mlm_score.decoder.weight', 'mlm_score.transform.LayerNorm.weight', 'mlm_score.transform.dense.bias', 'mlm_score.bias', 'mlm_score.transform.dense.weight']\n",
      "- This IS expected if you are initializing ViltModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViltModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViltProcessor, ViltModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "model = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058e7dd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m image \u001b[39m=\u001b[39m train_data[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m:\u001b[39m3\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m image \u001b[39m=\u001b[39m my_transforms(image)\n\u001b[1;32m      3\u001b[0m text \u001b[39m=\u001b[39m train_data[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m:\u001b[39m3\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/ml/venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Desktop/ml/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ml/venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:346\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    339\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/Desktop/ml/venv/lib/python3.9/site-packages/torchvision/transforms/functional.py:462\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    457\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    460\u001b[0m         )\n\u001b[0;32m--> 462\u001b[0m _, image_height, image_width \u001b[39m=\u001b[39m get_dimensions(img)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(size, \u001b[39mint\u001b[39m):\n\u001b[1;32m    464\u001b[0m     size \u001b[39m=\u001b[39m [size]\n",
      "File \u001b[0;32m~/Desktop/ml/venv/lib/python3.9/site-packages/torchvision/transforms/functional.py:75\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mget_dimensions(img)\n\u001b[0;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mget_dimensions(img)\n",
      "File \u001b[0;32m~/Desktop/ml/venv/lib/python3.9/site-packages/torchvision/transforms/functional_pil.py:33\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     31\u001b[0m     width, height \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msize\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m [channels, height, width]\n\u001b[0;32m---> 33\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'list'>"
     ]
    }
   ],
   "source": [
    "image = train_data['train'][0:3]['image']\n",
    "image = my_transforms(image)\n",
    "text = train_data['train'][0:3]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6d974bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(image, text, return_tensors=\"pt\")\n",
    "out = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f96c28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here is the code for adding a classification layer to the VilBert model\n",
    "\"\"\"\n",
    "class classificationVILT(torch.nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.projection = nn.Linear(768, 2)\n",
    "        self.classification = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, pixel_values, pixel_mask):\n",
    "        outputs = self.bert(input_ids, token_type_ids, attention_mask, pixel_values, pixel_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        to_feed = self.projection(pooled_output)\n",
    "        logits = self.classification(to_feed)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3aa81",
   "metadata": {},
   "source": [
    "print(outputs[0].shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafe555",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, epochs, lr, train_data, test_data, model, processor):\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.BCELoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def plot(self, loss):\n",
    "        timesteps = np.arange(1, loss.shape[0] + 1)\n",
    "        # Plot the MSE vs timesteps\n",
    "        plt.plot(timesteps, loss)\n",
    "        # Add axis labels and a title\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "        plt.title('Mean Squared Error over Timesteps')\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        adam = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        cosine = torch.optim.lr_scheduler.CosineAnnealingLR(adam, self.epochs)\n",
    "        loss_fct = nn.BCELoss()\n",
    "        accuracy = Accuracy(task='multiclass', num_classes=2).to(device)\n",
    "        mcc = MatthewsCorrCoef(task='binary').to(device)\n",
    "        counter = 0\n",
    "\n",
    "        training_loss_over_epochs = []\n",
    "        for epoch in range(self.epochs):\n",
    "            training_loss = []\n",
    "            total_acc = 0\n",
    "            num_train_steps = train_data['train'].num_rows\n",
    "            for train_index in range(num_train_steps):\n",
    "                model.zero_grad()\n",
    "                image = train_data['train'][train_index]['image']\n",
    "                text = train_data['train'][train_index]['text']\n",
    "                inputs = processor(image, text, return_tensors=\"pt\")\n",
    "                out = model(**inputs)\n",
    "                if(train_data['train'][train_index]['label'] == 0):\n",
    "                    truth = torch.tensor([0, train_data['train'][train_index]['label']], dtype=float)\n",
    "                else: \n",
    "                    truth = torch.tensor([1, train_data['train'][train_index]['label']], dtype=float)\n",
    "                loss = loss_fct(out.float(), truth.view(1, 2).float())\n",
    "                training_loss.append(loss.item())\n",
    "                maximums = torch.argmax(out)\n",
    "                truth_max = torch.argmax(truth)\n",
    "                # here is the accuracy measurement\n",
    "                #acc = accuracy(maximums, truth_max)\n",
    "                total_acc += (maximums == truth_max)\n",
    "                adam.zero_grad()\n",
    "                loss.backward()\n",
    "                adam.step()\n",
    "            self.plot(training_loss)\n",
    "            print('\\n')\n",
    "            print('epoch: ', counter)\n",
    "            counter += 1\n",
    "            print('training set accuracy: ', total_acc/train_index)\n",
    "            # the average matthews correlation coefficient?\n",
    "            #print('matthews correlation coefficient', total_mc/train_index)\n",
    "                # the total matthews correlation coefficient\n",
    "            #print('total matthews correlation coefficient', total_mc)\n",
    "            print('loss total: ', sum(training_loss))\n",
    "            print('\\n')\n",
    "            training_loss_over_epochs.append(training_loss)\n",
    "            #exponential.step()\n",
    "            cosine.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a1d03372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dandelin/vilt-b32-mlm were not used when initializing ViltModel: ['mlm_score.transform.dense.weight', 'mlm_score.bias', 'mlm_score.transform.LayerNorm.weight', 'mlm_score.transform.LayerNorm.bias', 'mlm_score.transform.dense.bias', 'mlm_score.decoder.weight']\n",
      "- This IS expected if you are initializing ViltModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViltModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "vilt = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d68849cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classificationVILT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m  classificationVILT(vilt)\n\u001b[1;32m      2\u001b[0m inputs \u001b[39m=\u001b[39m processor(image, text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m out \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classificationVILT' is not defined"
     ]
    }
   ],
   "source": [
    "model =  classificationVILT(vilt)\n",
    "inputs = processor(image, text, return_tensors=\"pt\")\n",
    "out = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b78a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = CustomTrainer(10, 1e-3, train_data, test_data, model, processor)\n",
    "train.train(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "42b2ed5eaf5b0df0635e21b6d1cbf29a12944d3072d5b8f2a0ee60d1c4610543"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
